_target_: models.smiles_transformer.SmilesTransformer

# Parameters

input_dim: 1
model_dim: 64
num_heads: 4
encoder_depth: 4
decoder_depth: 4
rows: 80
cols: 160
dropout: 0.0
target_vocab_size: 7

# General Parameters

optimizer:
  _target_: torch.optim.Adam
  _partial_: True
  lr: 1.e-4
  betas: [0.9, 0.999] # default
  weight_decay: 0.0 # default

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  _partial_: True
  T_0: 10
  T_mult: 2
  eta_min: 1.e-5

loss_func: 'bce'
seq_len: 500
num_atom_types: 7