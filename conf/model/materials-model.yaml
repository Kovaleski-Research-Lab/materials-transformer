_target_: models.smiles_transformer.SmilesTransformer

# Parameters

input_dim: 1
model_dim: 512
num_heads: 8
encoder_depth: 4
decoder_depth: 4
rows: 80
cols: 160
dropout: 0.05
vocab_size: 0
#target_vocab_size: 36 # 36 for sim data for now # 26 # 23 unique chars in the dataset + 3 specials
mcl_params: # weighting params for loss terms
  alpha: 1.0 # cross entropy
  beta: 0.0 # molecular weight
  gamma: 0.0 # rl
  delta: 0.0 # stoichiometry

# General Parameters

optimizer:
  _target_: torch.optim.Adam
  _partial_: True
  lr: 1.e-4
  betas: [0.9, 0.999] # default
  weight_decay: 0.0 # default

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  _partial_: True
  T_0: 10
  T_mult: 1
  eta_min: 1.e-6

loss_func: 'ce' # or 'rl' or 'mol' or **'multi'**
testing_method: 'beam' # or 'deterministic' or 'sampling'
testing_sample_num: 10 # doubles as beam width param for beam search