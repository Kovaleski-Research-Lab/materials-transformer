_target_: pytorch_lightning.Trainer

accelerator: auto
devices: auto
max_epochs: 2
precision: "32" #"16-mixed"
log_every_n_steps: 1

logger:
  _target_: pytorch_lightning.loggers.MLFlowLogger
  experiment_name: ${project_name}
  run_name: test01
  tracking_uri: "file://${paths.results}/mlruns"
  log_model: false # ModelCheckpoint callback handles this

# --- Callbacks Configuration ---
callbacks:
  # 1. Model Checkpointing
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val_loss"
    mode: "min"
    save_top_k: 1
    #dirpath: "${paths.results}/checkpoints/" wanna let MLflow handle this
    filename: "epoch_{epoch}-val_loss_{val_loss:.2f}"
    auto_insert_metric_name: False

  # 2. Early Stopping
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val_loss"
    mode: "min"
    patience: 15

  # 3. Learning Rate Monitoring
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "step"

  # 4. (Custom) Smiles Pred Logger
  - _target_: utils.callbacks.SMILES_Prediction_Logger